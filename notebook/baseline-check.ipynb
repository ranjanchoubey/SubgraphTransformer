{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ranjan/SubgraphTransformer/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "from dgl.data import CoraGraphDataset\n",
    "\n",
    "dataset = CoraGraphDataset()\n",
    "g = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(g, x)\n",
    "        x = F.relu(x)\n",
    "        # Dropout helps regularize\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        x = self.conv2(g, x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss 1.9462 | Train Acc 0.3500 | Val Acc 0.2220 | Test Acc 0.2180\n",
      "Epoch 001 | Loss 1.9335 | Train Acc 0.7143 | Val Acc 0.3600 | Test Acc 0.3730\n",
      "Epoch 002 | Loss 1.9193 | Train Acc 0.7357 | Val Acc 0.4180 | Test Acc 0.4370\n",
      "Epoch 003 | Loss 1.9027 | Train Acc 0.6643 | Val Acc 0.4020 | Test Acc 0.4020\n",
      "Epoch 004 | Loss 1.8803 | Train Acc 0.6500 | Val Acc 0.4040 | Test Acc 0.4260\n",
      "Epoch 005 | Loss 1.8587 | Train Acc 0.7429 | Val Acc 0.4580 | Test Acc 0.4840\n",
      "Epoch 006 | Loss 1.8399 | Train Acc 0.8214 | Val Acc 0.5160 | Test Acc 0.5530\n",
      "Epoch 007 | Loss 1.8111 | Train Acc 0.8714 | Val Acc 0.5860 | Test Acc 0.5870\n",
      "Epoch 008 | Loss 1.7872 | Train Acc 0.8929 | Val Acc 0.6220 | Test Acc 0.6240\n",
      "Epoch 009 | Loss 1.7691 | Train Acc 0.9071 | Val Acc 0.6500 | Test Acc 0.6480\n",
      "Epoch 010 | Loss 1.7306 | Train Acc 0.9357 | Val Acc 0.6720 | Test Acc 0.6810\n",
      "Epoch 011 | Loss 1.6984 | Train Acc 0.9357 | Val Acc 0.6780 | Test Acc 0.6980\n",
      "Epoch 012 | Loss 1.6583 | Train Acc 0.9357 | Val Acc 0.6880 | Test Acc 0.7020\n",
      "Epoch 013 | Loss 1.6329 | Train Acc 0.9429 | Val Acc 0.7140 | Test Acc 0.7200\n",
      "Epoch 014 | Loss 1.5968 | Train Acc 0.9429 | Val Acc 0.7220 | Test Acc 0.7440\n",
      "Epoch 015 | Loss 1.5621 | Train Acc 0.9571 | Val Acc 0.7320 | Test Acc 0.7600\n",
      "Epoch 016 | Loss 1.5328 | Train Acc 0.9643 | Val Acc 0.7380 | Test Acc 0.7680\n",
      "Epoch 017 | Loss 1.4969 | Train Acc 0.9643 | Val Acc 0.7420 | Test Acc 0.7760\n",
      "Epoch 018 | Loss 1.4410 | Train Acc 0.9643 | Val Acc 0.7420 | Test Acc 0.7810\n",
      "Epoch 019 | Loss 1.3904 | Train Acc 0.9643 | Val Acc 0.7480 | Test Acc 0.7830\n",
      "Epoch 020 | Loss 1.3664 | Train Acc 0.9643 | Val Acc 0.7520 | Test Acc 0.7840\n",
      "Epoch 021 | Loss 1.3208 | Train Acc 0.9643 | Val Acc 0.7540 | Test Acc 0.7860\n",
      "Epoch 022 | Loss 1.2838 | Train Acc 0.9643 | Val Acc 0.7520 | Test Acc 0.7860\n",
      "Epoch 023 | Loss 1.2318 | Train Acc 0.9643 | Val Acc 0.7600 | Test Acc 0.7860\n",
      "Epoch 024 | Loss 1.2110 | Train Acc 0.9714 | Val Acc 0.7620 | Test Acc 0.7870\n",
      "Epoch 025 | Loss 1.1656 | Train Acc 0.9714 | Val Acc 0.7600 | Test Acc 0.7900\n",
      "Epoch 026 | Loss 1.1407 | Train Acc 0.9786 | Val Acc 0.7560 | Test Acc 0.7890\n",
      "Epoch 027 | Loss 1.0748 | Train Acc 0.9786 | Val Acc 0.7640 | Test Acc 0.7930\n",
      "Epoch 028 | Loss 1.0258 | Train Acc 0.9786 | Val Acc 0.7700 | Test Acc 0.7960\n",
      "Epoch 029 | Loss 1.0153 | Train Acc 0.9786 | Val Acc 0.7740 | Test Acc 0.7990\n",
      "Epoch 030 | Loss 0.9674 | Train Acc 0.9714 | Val Acc 0.7740 | Test Acc 0.8010\n",
      "Epoch 031 | Loss 0.9178 | Train Acc 0.9786 | Val Acc 0.7820 | Test Acc 0.8100\n",
      "Epoch 032 | Loss 0.9079 | Train Acc 0.9786 | Val Acc 0.7840 | Test Acc 0.8120\n",
      "Epoch 033 | Loss 0.8825 | Train Acc 0.9786 | Val Acc 0.7780 | Test Acc 0.8090\n",
      "Epoch 034 | Loss 0.8069 | Train Acc 0.9786 | Val Acc 0.7840 | Test Acc 0.8090\n",
      "Epoch 035 | Loss 0.7729 | Train Acc 0.9786 | Val Acc 0.7820 | Test Acc 0.8120\n",
      "Epoch 036 | Loss 0.7786 | Train Acc 0.9786 | Val Acc 0.7820 | Test Acc 0.8110\n",
      "Epoch 037 | Loss 0.7310 | Train Acc 0.9786 | Val Acc 0.7840 | Test Acc 0.8110\n",
      "Epoch 038 | Loss 0.7177 | Train Acc 0.9786 | Val Acc 0.7820 | Test Acc 0.8110\n",
      "Epoch 039 | Loss 0.6942 | Train Acc 0.9786 | Val Acc 0.7800 | Test Acc 0.8110\n",
      "Epoch 040 | Loss 0.6757 | Train Acc 0.9786 | Val Acc 0.7820 | Test Acc 0.8090\n",
      "Epoch 041 | Loss 0.6434 | Train Acc 0.9786 | Val Acc 0.7900 | Test Acc 0.8130\n",
      "Epoch 042 | Loss 0.6024 | Train Acc 0.9786 | Val Acc 0.7940 | Test Acc 0.8170\n",
      "Epoch 043 | Loss 0.5890 | Train Acc 0.9786 | Val Acc 0.7920 | Test Acc 0.8170\n",
      "Epoch 044 | Loss 0.5783 | Train Acc 0.9786 | Val Acc 0.7940 | Test Acc 0.8130\n",
      "Epoch 045 | Loss 0.5733 | Train Acc 0.9786 | Val Acc 0.7840 | Test Acc 0.8140\n",
      "Epoch 046 | Loss 0.5662 | Train Acc 0.9786 | Val Acc 0.7780 | Test Acc 0.8080\n",
      "Epoch 047 | Loss 0.5249 | Train Acc 0.9857 | Val Acc 0.7800 | Test Acc 0.8090\n",
      "Epoch 048 | Loss 0.5154 | Train Acc 0.9857 | Val Acc 0.7820 | Test Acc 0.8090\n",
      "Epoch 049 | Loss 0.4921 | Train Acc 0.9857 | Val Acc 0.7820 | Test Acc 0.8100\n",
      "Epoch 050 | Loss 0.5026 | Train Acc 0.9857 | Val Acc 0.7820 | Test Acc 0.8150\n",
      "Epoch 051 | Loss 0.4839 | Train Acc 0.9857 | Val Acc 0.7860 | Test Acc 0.8190\n",
      "Epoch 052 | Loss 0.4617 | Train Acc 0.9857 | Val Acc 0.7880 | Test Acc 0.8170\n",
      "Epoch 053 | Loss 0.4511 | Train Acc 0.9857 | Val Acc 0.7840 | Test Acc 0.8160\n",
      "Epoch 054 | Loss 0.4589 | Train Acc 0.9857 | Val Acc 0.7920 | Test Acc 0.8130\n",
      "Epoch 055 | Loss 0.4427 | Train Acc 0.9929 | Val Acc 0.7900 | Test Acc 0.8120\n",
      "Epoch 056 | Loss 0.4389 | Train Acc 0.9929 | Val Acc 0.7880 | Test Acc 0.8100\n",
      "Epoch 057 | Loss 0.4281 | Train Acc 0.9929 | Val Acc 0.7820 | Test Acc 0.8080\n",
      "Epoch 058 | Loss 0.4267 | Train Acc 0.9929 | Val Acc 0.7800 | Test Acc 0.8070\n",
      "Epoch 059 | Loss 0.4007 | Train Acc 0.9929 | Val Acc 0.7780 | Test Acc 0.8090\n",
      "Epoch 060 | Loss 0.3899 | Train Acc 0.9929 | Val Acc 0.7860 | Test Acc 0.8100\n",
      "Epoch 061 | Loss 0.3733 | Train Acc 0.9929 | Val Acc 0.7860 | Test Acc 0.8110\n",
      "Epoch 062 | Loss 0.3917 | Train Acc 0.9929 | Val Acc 0.7840 | Test Acc 0.8130\n",
      "Epoch 063 | Loss 0.3745 | Train Acc 0.9929 | Val Acc 0.7860 | Test Acc 0.8110\n",
      "Epoch 064 | Loss 0.3854 | Train Acc 0.9929 | Val Acc 0.7880 | Test Acc 0.8090\n",
      "Epoch 065 | Loss 0.3752 | Train Acc 0.9929 | Val Acc 0.7900 | Test Acc 0.8080\n",
      "Epoch 066 | Loss 0.3920 | Train Acc 0.9929 | Val Acc 0.7900 | Test Acc 0.8070\n",
      "Epoch 067 | Loss 0.3667 | Train Acc 0.9929 | Val Acc 0.7840 | Test Acc 0.8070\n",
      "Epoch 068 | Loss 0.3406 | Train Acc 0.9929 | Val Acc 0.7820 | Test Acc 0.8110\n",
      "Epoch 069 | Loss 0.3507 | Train Acc 0.9929 | Val Acc 0.7860 | Test Acc 0.8130\n",
      "Epoch 070 | Loss 0.3790 | Train Acc 0.9929 | Val Acc 0.7860 | Test Acc 0.8110\n",
      "Epoch 071 | Loss 0.3423 | Train Acc 0.9929 | Val Acc 0.7920 | Test Acc 0.8140\n",
      "Epoch 072 | Loss 0.3325 | Train Acc 0.9929 | Val Acc 0.7940 | Test Acc 0.8100\n",
      "Epoch 073 | Loss 0.3474 | Train Acc 0.9929 | Val Acc 0.7880 | Test Acc 0.8090\n",
      "Epoch 074 | Loss 0.3323 | Train Acc 0.9929 | Val Acc 0.7840 | Test Acc 0.8070\n",
      "Epoch 075 | Loss 0.3239 | Train Acc 0.9929 | Val Acc 0.7760 | Test Acc 0.8050\n",
      "Epoch 076 | Loss 0.3155 | Train Acc 0.9929 | Val Acc 0.7780 | Test Acc 0.8090\n",
      "Epoch 077 | Loss 0.3246 | Train Acc 0.9929 | Val Acc 0.7780 | Test Acc 0.8120\n",
      "Epoch 078 | Loss 0.3351 | Train Acc 0.9929 | Val Acc 0.7800 | Test Acc 0.8120\n",
      "Epoch 079 | Loss 0.3033 | Train Acc 0.9929 | Val Acc 0.7820 | Test Acc 0.8130\n",
      "Epoch 080 | Loss 0.3130 | Train Acc 0.9929 | Val Acc 0.7840 | Test Acc 0.8060\n",
      "Epoch 081 | Loss 0.3061 | Train Acc 0.9929 | Val Acc 0.7820 | Test Acc 0.7990\n",
      "Epoch 082 | Loss 0.3154 | Train Acc 0.9929 | Val Acc 0.7800 | Test Acc 0.7970\n",
      "Epoch 083 | Loss 0.3121 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.7990\n",
      "Epoch 084 | Loss 0.3058 | Train Acc 0.9929 | Val Acc 0.7840 | Test Acc 0.7990\n",
      "Epoch 085 | Loss 0.2896 | Train Acc 0.9929 | Val Acc 0.7840 | Test Acc 0.8030\n",
      "Epoch 086 | Loss 0.3067 | Train Acc 0.9929 | Val Acc 0.7780 | Test Acc 0.8090\n",
      "Epoch 087 | Loss 0.2846 | Train Acc 0.9929 | Val Acc 0.7800 | Test Acc 0.8080\n",
      "Epoch 088 | Loss 0.2991 | Train Acc 0.9929 | Val Acc 0.7840 | Test Acc 0.8110\n",
      "Epoch 089 | Loss 0.2877 | Train Acc 0.9929 | Val Acc 0.7900 | Test Acc 0.8120\n",
      "Epoch 090 | Loss 0.2839 | Train Acc 0.9929 | Val Acc 0.7920 | Test Acc 0.8070\n",
      "Epoch 091 | Loss 0.2606 | Train Acc 0.9929 | Val Acc 0.7880 | Test Acc 0.8050\n",
      "Epoch 092 | Loss 0.3144 | Train Acc 1.0000 | Val Acc 0.7820 | Test Acc 0.8000\n",
      "Epoch 093 | Loss 0.2809 | Train Acc 1.0000 | Val Acc 0.7800 | Test Acc 0.7990\n",
      "Epoch 094 | Loss 0.2916 | Train Acc 1.0000 | Val Acc 0.7760 | Test Acc 0.7970\n",
      "Epoch 095 | Loss 0.2722 | Train Acc 1.0000 | Val Acc 0.7760 | Test Acc 0.7990\n",
      "Epoch 096 | Loss 0.2888 | Train Acc 0.9929 | Val Acc 0.7820 | Test Acc 0.8030\n",
      "Epoch 097 | Loss 0.2785 | Train Acc 0.9929 | Val Acc 0.7860 | Test Acc 0.8100\n",
      "Epoch 098 | Loss 0.2950 | Train Acc 0.9929 | Val Acc 0.7880 | Test Acc 0.8150\n",
      "Epoch 099 | Loss 0.2708 | Train Acc 0.9929 | Val Acc 0.7880 | Test Acc 0.8120\n",
      "Epoch 100 | Loss 0.2660 | Train Acc 0.9929 | Val Acc 0.7860 | Test Acc 0.8060\n",
      "Epoch 101 | Loss 0.2721 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.8030\n",
      "Epoch 102 | Loss 0.2666 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.8010\n",
      "Epoch 103 | Loss 0.2581 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.7960\n",
      "Epoch 104 | Loss 0.2327 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.8000\n",
      "Epoch 105 | Loss 0.2581 | Train Acc 1.0000 | Val Acc 0.7820 | Test Acc 0.8040\n",
      "Epoch 106 | Loss 0.2515 | Train Acc 0.9929 | Val Acc 0.7840 | Test Acc 0.8070\n",
      "Epoch 107 | Loss 0.2370 | Train Acc 0.9929 | Val Acc 0.7860 | Test Acc 0.8110\n",
      "Epoch 108 | Loss 0.2458 | Train Acc 0.9929 | Val Acc 0.7900 | Test Acc 0.8110\n",
      "Epoch 109 | Loss 0.2533 | Train Acc 0.9929 | Val Acc 0.7900 | Test Acc 0.8070\n",
      "Epoch 110 | Loss 0.2460 | Train Acc 0.9929 | Val Acc 0.7900 | Test Acc 0.8060\n",
      "Epoch 111 | Loss 0.2434 | Train Acc 0.9929 | Val Acc 0.7840 | Test Acc 0.8040\n",
      "Epoch 112 | Loss 0.2531 | Train Acc 0.9929 | Val Acc 0.7840 | Test Acc 0.8030\n",
      "Epoch 113 | Loss 0.2369 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.8030\n",
      "Epoch 114 | Loss 0.2343 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.8000\n",
      "Epoch 115 | Loss 0.2299 | Train Acc 1.0000 | Val Acc 0.7940 | Test Acc 0.8010\n",
      "Epoch 116 | Loss 0.2300 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.8010\n",
      "Epoch 117 | Loss 0.2510 | Train Acc 1.0000 | Val Acc 0.7900 | Test Acc 0.8000\n",
      "Epoch 118 | Loss 0.2447 | Train Acc 1.0000 | Val Acc 0.7860 | Test Acc 0.8030\n",
      "Epoch 119 | Loss 0.2380 | Train Acc 0.9929 | Val Acc 0.7880 | Test Acc 0.8100\n",
      "Epoch 120 | Loss 0.2538 | Train Acc 0.9929 | Val Acc 0.7880 | Test Acc 0.8090\n",
      "Epoch 121 | Loss 0.2340 | Train Acc 1.0000 | Val Acc 0.7880 | Test Acc 0.8060\n",
      "Epoch 122 | Loss 0.2296 | Train Acc 1.0000 | Val Acc 0.7860 | Test Acc 0.8040\n",
      "Epoch 123 | Loss 0.2304 | Train Acc 1.0000 | Val Acc 0.7900 | Test Acc 0.8030\n",
      "Epoch 124 | Loss 0.2107 | Train Acc 1.0000 | Val Acc 0.7880 | Test Acc 0.8050\n",
      "Epoch 125 | Loss 0.2357 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.8060\n",
      "Epoch 126 | Loss 0.2223 | Train Acc 1.0000 | Val Acc 0.7860 | Test Acc 0.8030\n",
      "Epoch 127 | Loss 0.2195 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.8010\n",
      "Epoch 128 | Loss 0.2304 | Train Acc 1.0000 | Val Acc 0.7780 | Test Acc 0.8010\n",
      "Epoch 129 | Loss 0.2318 | Train Acc 1.0000 | Val Acc 0.7780 | Test Acc 0.8030\n",
      "Epoch 130 | Loss 0.2317 | Train Acc 1.0000 | Val Acc 0.7860 | Test Acc 0.8040\n",
      "Epoch 131 | Loss 0.2304 | Train Acc 1.0000 | Val Acc 0.7880 | Test Acc 0.8040\n",
      "Epoch 132 | Loss 0.2210 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.8030\n",
      "Epoch 133 | Loss 0.2311 | Train Acc 1.0000 | Val Acc 0.7980 | Test Acc 0.8050\n",
      "Epoch 134 | Loss 0.2183 | Train Acc 1.0000 | Val Acc 0.7980 | Test Acc 0.8030\n",
      "Epoch 135 | Loss 0.2205 | Train Acc 1.0000 | Val Acc 0.7940 | Test Acc 0.8060\n",
      "Epoch 136 | Loss 0.2089 | Train Acc 1.0000 | Val Acc 0.7940 | Test Acc 0.8030\n",
      "Epoch 137 | Loss 0.2190 | Train Acc 1.0000 | Val Acc 0.7860 | Test Acc 0.8020\n",
      "Epoch 138 | Loss 0.1969 | Train Acc 1.0000 | Val Acc 0.7820 | Test Acc 0.7990\n",
      "Epoch 139 | Loss 0.2305 | Train Acc 1.0000 | Val Acc 0.7780 | Test Acc 0.7930\n",
      "Epoch 140 | Loss 0.2320 | Train Acc 1.0000 | Val Acc 0.7880 | Test Acc 0.7950\n",
      "Epoch 141 | Loss 0.2163 | Train Acc 1.0000 | Val Acc 0.7880 | Test Acc 0.8040\n",
      "Epoch 142 | Loss 0.1915 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.8050\n",
      "Epoch 143 | Loss 0.2177 | Train Acc 1.0000 | Val Acc 0.7940 | Test Acc 0.8070\n",
      "Epoch 144 | Loss 0.1986 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.8000\n",
      "Epoch 145 | Loss 0.2283 | Train Acc 1.0000 | Val Acc 0.7900 | Test Acc 0.8020\n",
      "Epoch 146 | Loss 0.2238 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.8020\n",
      "Epoch 147 | Loss 0.2275 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.8000\n",
      "Epoch 148 | Loss 0.2061 | Train Acc 1.0000 | Val Acc 0.7880 | Test Acc 0.8010\n",
      "Epoch 149 | Loss 0.2032 | Train Acc 1.0000 | Val Acc 0.7860 | Test Acc 0.8010\n",
      "Epoch 150 | Loss 0.2094 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.8040\n",
      "Epoch 151 | Loss 0.2020 | Train Acc 1.0000 | Val Acc 0.7940 | Test Acc 0.8030\n",
      "Epoch 152 | Loss 0.2190 | Train Acc 1.0000 | Val Acc 0.7960 | Test Acc 0.8060\n",
      "Epoch 153 | Loss 0.2078 | Train Acc 1.0000 | Val Acc 0.7940 | Test Acc 0.8070\n",
      "Epoch 154 | Loss 0.1788 | Train Acc 1.0000 | Val Acc 0.7940 | Test Acc 0.8070\n",
      "Epoch 155 | Loss 0.1853 | Train Acc 1.0000 | Val Acc 0.7960 | Test Acc 0.8080\n",
      "Epoch 156 | Loss 0.2047 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.8040\n",
      "Epoch 157 | Loss 0.1999 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.8000\n",
      "Epoch 158 | Loss 0.2113 | Train Acc 1.0000 | Val Acc 0.7740 | Test Acc 0.8010\n",
      "Epoch 159 | Loss 0.1991 | Train Acc 1.0000 | Val Acc 0.7780 | Test Acc 0.8020\n",
      "Epoch 160 | Loss 0.2047 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.7910\n",
      "Epoch 161 | Loss 0.2058 | Train Acc 1.0000 | Val Acc 0.7900 | Test Acc 0.7970\n",
      "Epoch 162 | Loss 0.1929 | Train Acc 1.0000 | Val Acc 0.7940 | Test Acc 0.8000\n",
      "Epoch 163 | Loss 0.2056 | Train Acc 1.0000 | Val Acc 0.7960 | Test Acc 0.8040\n",
      "Epoch 164 | Loss 0.2094 | Train Acc 1.0000 | Val Acc 0.7980 | Test Acc 0.8050\n",
      "Epoch 165 | Loss 0.1865 | Train Acc 1.0000 | Val Acc 0.8020 | Test Acc 0.8060\n",
      "Epoch 166 | Loss 0.2057 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.8030\n",
      "Epoch 167 | Loss 0.2127 | Train Acc 1.0000 | Val Acc 0.7940 | Test Acc 0.8010\n",
      "Epoch 168 | Loss 0.2129 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.8010\n",
      "Epoch 169 | Loss 0.1979 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.8020\n",
      "Epoch 170 | Loss 0.1841 | Train Acc 1.0000 | Val Acc 0.7860 | Test Acc 0.7940\n",
      "Epoch 171 | Loss 0.1863 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.7950\n",
      "Epoch 172 | Loss 0.1757 | Train Acc 1.0000 | Val Acc 0.7940 | Test Acc 0.8040\n",
      "Epoch 173 | Loss 0.2178 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.8010\n",
      "Epoch 174 | Loss 0.2028 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.8040\n",
      "Epoch 175 | Loss 0.1907 | Train Acc 1.0000 | Val Acc 0.7960 | Test Acc 0.8050\n",
      "Epoch 176 | Loss 0.1698 | Train Acc 1.0000 | Val Acc 0.7980 | Test Acc 0.8010\n",
      "Epoch 177 | Loss 0.1876 | Train Acc 1.0000 | Val Acc 0.8040 | Test Acc 0.8000\n",
      "Epoch 178 | Loss 0.1892 | Train Acc 1.0000 | Val Acc 0.8020 | Test Acc 0.8000\n",
      "Epoch 179 | Loss 0.1998 | Train Acc 1.0000 | Val Acc 0.8020 | Test Acc 0.8010\n",
      "Epoch 180 | Loss 0.1842 | Train Acc 1.0000 | Val Acc 0.8000 | Test Acc 0.8050\n",
      "Epoch 181 | Loss 0.1963 | Train Acc 1.0000 | Val Acc 0.8000 | Test Acc 0.8070\n",
      "Epoch 182 | Loss 0.1911 | Train Acc 1.0000 | Val Acc 0.7940 | Test Acc 0.8040\n",
      "Epoch 183 | Loss 0.1782 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.8040\n",
      "Epoch 184 | Loss 0.1975 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.8050\n",
      "Epoch 185 | Loss 0.1690 | Train Acc 1.0000 | Val Acc 0.7900 | Test Acc 0.8000\n",
      "Epoch 186 | Loss 0.1767 | Train Acc 1.0000 | Val Acc 0.7800 | Test Acc 0.7960\n",
      "Epoch 187 | Loss 0.1689 | Train Acc 1.0000 | Val Acc 0.7740 | Test Acc 0.7900\n",
      "Epoch 188 | Loss 0.1776 | Train Acc 1.0000 | Val Acc 0.7800 | Test Acc 0.7910\n",
      "Epoch 189 | Loss 0.1748 | Train Acc 1.0000 | Val Acc 0.7920 | Test Acc 0.7980\n",
      "Epoch 190 | Loss 0.1701 | Train Acc 1.0000 | Val Acc 0.7980 | Test Acc 0.8020\n",
      "Epoch 191 | Loss 0.1911 | Train Acc 1.0000 | Val Acc 0.8000 | Test Acc 0.8020\n",
      "Epoch 192 | Loss 0.1869 | Train Acc 1.0000 | Val Acc 0.7980 | Test Acc 0.8050\n",
      "Epoch 193 | Loss 0.1869 | Train Acc 1.0000 | Val Acc 0.7980 | Test Acc 0.8020\n",
      "Epoch 194 | Loss 0.1748 | Train Acc 1.0000 | Val Acc 0.7880 | Test Acc 0.8070\n",
      "Epoch 195 | Loss 0.1783 | Train Acc 1.0000 | Val Acc 0.7800 | Test Acc 0.8050\n",
      "Epoch 196 | Loss 0.1842 | Train Acc 1.0000 | Val Acc 0.7720 | Test Acc 0.7990\n",
      "Epoch 197 | Loss 0.1831 | Train Acc 1.0000 | Val Acc 0.7780 | Test Acc 0.7970\n",
      "Epoch 198 | Loss 0.1786 | Train Acc 1.0000 | Val Acc 0.7840 | Test Acc 0.7990\n",
      "Epoch 199 | Loss 0.1949 | Train Acc 1.0000 | Val Acc 0.7980 | Test Acc 0.7950\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "g = g.to(device)\n",
    "\n",
    "features = g.ndata['feat'].to(device)\n",
    "labels = g.ndata['label'].to(device)\n",
    "train_mask = g.ndata['train_mask'].to(device)\n",
    "val_mask = g.ndata['val_mask'].to(device)\n",
    "test_mask = g.ndata['test_mask'].to(device)\n",
    "\n",
    "in_feats = features.shape[1]\n",
    "num_classes = dataset.num_classes\n",
    "hidden_feats = 64\n",
    "\n",
    "model = GCN(in_feats, hidden_feats, num_classes, dropout=0.5).to(device)\n",
    "\n",
    "# Typical Adam optimizer with weight decay\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    logits = model(g, features)  # [N, num_classes]\n",
    "    loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        \n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d} | \"\n",
    "        f\"Loss {loss.item():.4f} | \"\n",
    "        f\"Train Acc {train_acc.item():.4f} | \"\n",
    "        f\"Val Acc {val_acc.item():.4f} | \"\n",
    "        f\"Test Acc {test_acc.item():.4f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
