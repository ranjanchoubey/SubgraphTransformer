{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory of 'src' to the Python path\n",
    "sys.path.append('/ranjan/graphtransformer/my_project')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing import load_cora_data, partition_graph\n",
    "from src.embedding import mean_pooling, compute_laplacian_positional_embedding, compute_gcn_embeddings\n",
    "from src.transformer import GraphTransformer\n",
    "from src.trainer import train_model, evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load the Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded: Cora, Number of Graphs: 1\n",
      "Graph Info:\n",
      "Nodes: 2708, Edges: 10556, Features: 1433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = load_cora_data()\n",
    "print(f\"Graph Info:\\nNodes: {graph.num_nodes}, Edges: {graph.num_edges}, Features: {graph.num_node_features}\")\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  ..., False, False, False])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.train_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  `graph.train_mask` is likely a boolean mask indicating which nodes in the Cora dataset are used for training.\n",
    "- This mask is commonly used in machine learning tasks to separate the dataset into training, validation, and test sets.\n",
    "- By applying this mask, you can filter out the nodes that are designated for training purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Partition the graph into subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph partitioned into 10 subgraphs.\n",
      "Data(x=[277, 1433], y=[277], train_mask=[277], val_mask=[277], test_mask=[277], edge_index=[2, 1164])\n",
      "Data(x=[270, 1433], y=[270], train_mask=[270], val_mask=[270], test_mask=[270], edge_index=[2, 866])\n",
      "Data(x=[273, 1433], y=[273], train_mask=[273], val_mask=[273], test_mask=[273], edge_index=[2, 944])\n",
      "Data(x=[262, 1433], y=[262], train_mask=[262], val_mask=[262], test_mask=[262], edge_index=[2, 870])\n",
      "Data(x=[273, 1433], y=[273], train_mask=[273], val_mask=[273], test_mask=[273], edge_index=[2, 960])\n",
      "Data(x=[274, 1433], y=[274], train_mask=[274], val_mask=[274], test_mask=[274], edge_index=[2, 1140])\n",
      "Data(x=[262, 1433], y=[262], train_mask=[262], val_mask=[262], test_mask=[262], edge_index=[2, 740])\n",
      "Data(x=[265, 1433], y=[265], train_mask=[265], val_mask=[265], test_mask=[265], edge_index=[2, 812])\n",
      "Data(x=[277, 1433], y=[277], train_mask=[277], val_mask=[277], test_mask=[277], edge_index=[2, 980])\n",
      "Data(x=[275, 1433], y=[275], train_mask=[275], val_mask=[275], test_mask=[275], edge_index=[2, 906])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_parts = 10  # More partitions for larger training set\n",
    "cluster_data = partition_graph(graph, num_parts=num_parts)\n",
    "\n",
    "for subgraphs in cluster_data:\n",
    "    print(subgraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Compute embeddings and track mask information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_embeddings = []\n",
    "lpe_embeddings = []\n",
    "node_labels = []\n",
    "num_nodes_list = []\n",
    "train_masks = []\n",
    "val_masks = []\n",
    "test_masks = []\n",
    "\n",
    "for i in range(num_parts):\n",
    "    subgraph = cluster_data[i]\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Subgraph {i} - Number of nodes: {subgraph.num_nodes}\")\n",
    "    print(f\"Subgraph {i} - Feature vector size: {subgraph.x.size(1)}\")\n",
    "        \n",
    "    # Compute GCN embeddings\n",
    "    gcn_embeddings = compute_gcn_embeddings(subgraph, input_dim=1433, hidden_dim=64, output_dim=16)\n",
    "        \n",
    "    # Compute Laplacian positional embeddings\n",
    "    lpe = compute_laplacian_positional_embedding(subgraph, embedding_dim=16)\n",
    "        \n",
    "    # Compute subgraph-level embeddings using mean pooling\n",
    "    subgraph_embedding = mean_pooling(gcn_embeddings)\n",
    "        \n",
    "    # Append subgraph-level embeddings and labels\n",
    "    subgraph_embeddings.append(subgraph_embedding)\n",
    "    lpe_embeddings.append(lpe.mean(dim=0))  # Mean pooling for LPE as well\n",
    "    node_labels.append(subgraph.y)\n",
    "    num_nodes_list.append(subgraph.num_nodes)\n",
    "    \n",
    "    # Store the masks for each subgraph\n",
    "    train_masks.append(subgraph.train_mask)\n",
    "    val_masks.append(subgraph.val_mask)\n",
    "    test_masks.append(subgraph.test_mask)\n",
    "    \n",
    "    # Debugging prints to check tensor sizes\n",
    "    print(f\"Subgraph {i} - GCN Embeddings Size: {gcn_embeddings.size()}\")\n",
    "    print(f\"Subgraph {i} - LPE Size: {lpe.size()}\")\n",
    "    print(f\"Subgraph {i} - Subgraph Embedding Size: {subgraph_embedding.size()}\")\n",
    "    print(f\"Subgraph {i} - Node Labels Size: {subgraph.y.size()}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "subgraph_embeddings = torch.stack(subgraph_embeddings)\n",
    "lpe_embeddings = torch.stack(lpe_embeddings)\n",
    "node_labels = torch.cat(node_labels, dim=0)\n",
    "num_nodes_list = torch.tensor(num_nodes_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
